{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37a6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023dbc3d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "752804f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data1 = pd.read_csv('train_1.csv')\n",
    "df_data2 = pd.read_csv('train_2.csv')\n",
    "df_data3 = pd.read_csv('train_3.csv')\n",
    "df_data4 = pd.read_csv('train_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74330f9",
   "metadata": {},
   "source": [
    "## Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47592db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardsc(df):\n",
    "    data1 = sc.fit_transform(df.iloc[:,6:])\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16fb3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data1.iloc[:,6:] = standardsc(df_data1)\n",
    "df_data2.iloc[:,6:] = standardsc(df_data2)\n",
    "df_data3.iloc[:,6:] = standardsc(df_data3)\n",
    "df_data4.iloc[:,6:] = standardsc(df_data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5bbf85",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf02470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply one hot encoding method to country code\n",
    "def O_H_E_country(df):\n",
    "    Country = pd.get_dummies(df['Country_Code'])\n",
    "    df['Country_0']=  Country.iloc[:,0]\n",
    "    df['Country_1']=  Country.iloc[:,1]\n",
    "    df['Country_2']=  Country.iloc[:,2]\n",
    "    df = df.drop(['Country_2'],axis =1)\n",
    "    return df\n",
    "\n",
    "df_data1 = O_H_E_country(df_data1)\n",
    "df_data2 = O_H_E_country(df_data2)\n",
    "df_data3 = O_H_E_country(df_data3)\n",
    "df_data4 = O_H_E_country(df_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fc62e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the one hot encoding method to period variable and separated the columns by years\n",
    "def O_H_E_country_periods(df):   \n",
    "    Periods = pd.get_dummies(df['Period'])\n",
    "    Y2014Q4 = Periods.iloc[:,0]\n",
    "    Y2015 = Periods.iloc[:,1:5].max(axis = 1)\n",
    "    Y2016 = Periods.iloc[:,5:9].max(axis = 1)\n",
    "    Y2017 = Periods.iloc[:,9:13].max(axis = 1)\n",
    "    Y2018 = Periods.iloc[:,13:17].max(axis = 1)\n",
    "    Y2019 = Periods.iloc[:,17:21].max(axis = 1)\n",
    "    Y2020Q1 = Periods.iloc[:,21]\n",
    "    \n",
    "    df['Y2014Q4'] = Y2014Q4\n",
    "    df['Y2015'] = Y2015\n",
    "    df['Y2016'] = Y2016\n",
    "    df['Y2017'] = Y2017\n",
    "    df['Y2018'] = Y2018\n",
    "    df['Y2019'] = Y2019\n",
    "    df['Y2020Q1'] = Y2020Q1\n",
    "    df = df.drop(['Y2020Q1'],axis =1)\n",
    "    return df\n",
    "\n",
    "df_data1 = O_H_E_country_periods(df_data1)\n",
    "df_data2 = O_H_E_country_periods(df_data2)\n",
    "df_data3 = O_H_E_country_periods(df_data3)\n",
    "df_data4 = O_H_E_country_periods(df_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832bb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply one hot encoding method to BR code \n",
    "def O_H_E_BR(df): \n",
    "    BR_data = pd.get_dummies(df['BR Code'])\n",
    "    new_column_names = ['BR_Code_' + str(i) for i in range(len(BR_data.columns))]\n",
    "    BR_data_renamed = BR_data.rename(columns=dict(zip(BR_data.columns, new_column_names)))\n",
    "    BR_data_combined = pd.concat([df, BR_data_renamed], axis=1)\n",
    "    BR_data_combined = BR_data_combined.drop(['BR_Code_0'],axis =1)\n",
    "    return BR_data_combined\n",
    "\n",
    "df_data1 = O_H_E_BR(df_data1)\n",
    "df_data2 = O_H_E_BR(df_data2)\n",
    "df_data3 = O_H_E_BR(df_data3)\n",
    "df_data4 = O_H_E_BR(df_data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b8c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the orignal columns\n",
    "def drop_columns(df):\n",
    "    df =  df.drop(['BR Code','Country_Code','Period','Client','Self_exclude_flag'],axis =1)\n",
    "    return df\n",
    "\n",
    "df_data1 = drop_columns(df_data1)\n",
    "df_data2 = drop_columns(df_data2)\n",
    "df_data3 = drop_columns(df_data3)\n",
    "df_data4 = drop_columns(df_data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592830cc",
   "metadata": {},
   "source": [
    "## Create Target and input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bb5ac",
   "metadata": {},
   "source": [
    "To ensure that each target dataset after splitting contains only unique values in the 'risk-rating' column, we encounter an issue with dataset 1, as it contains two instances with a '1' value in this column. Consequently, we will handle dataset 1 differently, following our usual splitting method. However, for the other three datasets, we will employ a loop to identify a random state that satisfies the aforementioned condition during the splitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28300750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point\n",
    "data_1 = df_data1.copy()\n",
    "data_2 = df_data2.copy()\n",
    "data_3 = df_data3.copy()\n",
    "data_4 = df_data4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2309a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set the input and target\n",
    "inputs = data_1.iloc[:,1:]\n",
    "targets = data_1.iloc[:,:1]\n",
    "\n",
    "#Separated the dataset by 80-10-10 for dataset 1\n",
    "x_train1,x_valid1,y_train1,y_valid1 = train_test_split(inputs,targets,train_size = 0.9, random_state = 12)\n",
    "x_train1,x_test1,y_train1,y_test1 = train_test_split(x_train1,y_train1,test_size = 1/9 , random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ffbe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using loop the to ensure all unique in column risk-rating in three dataset\n",
    "def splitting(df):\n",
    "    for n in range(0,100):\n",
    "        x_train,x_valid,y_train,y_valid = train_test_split(inputs,targets,train_size = 0.9, random_state = n)\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x_train,y_train,test_size = 1/9 , random_state = 10)\n",
    "        if len(y_valid['risk_rating'].unique()) == 16 and len(y_test['risk_rating'].unique()) == 16 and len(y_train['risk_rating'].unique()) == 16:\n",
    "            break\n",
    "    return x_train,x_valid,y_train,y_valid,x_test,y_test\n",
    "\n",
    "x_train2,x_valid2,y_train2,y_valid2,x_test2,y_test2 = splitting(data_2)\n",
    "x_train3,x_valid3,y_train3,y_valid3,x_test3,y_test3 = splitting(data_3)\n",
    "x_train4,x_valid4,y_train4,y_valid4,x_test4,y_test4 = splitting(data_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116ff44",
   "metadata": {},
   "source": [
    "## Desicion tree for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8809fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with 1st dataset: 0.16963696369636963\n",
      "model with 2st dataset: 0.16435643564356436\n",
      "model with 3st dataset: 0.17227722772277226\n",
      "model with 4st dataset: 0.1808580858085809\n"
     ]
    }
   ],
   "source": [
    "#employed desicion tree to test the each dataset\n",
    "DT_1 = DecisionTreeClassifier()\n",
    "DT_1.fit(x_train1, y_train1)\n",
    "y_pred_1 = DT_1.predict(x_valid1)\n",
    "print(f\"model with 1st dataset: {f1_score(y_valid1, y_pred_1, average='micro')}\")\n",
    "\n",
    "DT_2 = DecisionTreeClassifier()\n",
    "DT_2.fit(x_train2, y_train2)\n",
    "y_pred_2 = DT_2.predict(x_valid2)\n",
    "print(f\"model with 2st dataset: {f1_score(y_valid2, y_pred_2, average='micro')}\")\n",
    "\n",
    "DT_3 = DecisionTreeClassifier()\n",
    "DT_3.fit(x_train3, y_train3)\n",
    "y_pred_3 = DT_3.predict(x_valid3)\n",
    "print(f\"model with 3st dataset: {f1_score(y_valid3, y_pred_3, average='micro')}\")\n",
    "\n",
    "DT_4 = DecisionTreeClassifier()\n",
    "DT_4.fit(x_train4, y_train4)\n",
    "y_pred_4 = DT_4.predict(x_valid4)\n",
    "print(f\"model with 4st dataset: {f1_score(y_valid4, y_pred_4, average='micro')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5122418",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(data ={'Drop_20': f1_score(y_valid1, y_pred_1, average='micro'),\n",
    "                     'mean': f1_score(y_valid2, y_pred_2, average='micro'),\n",
    "                     'median': f1_score(y_valid3, y_pred_3, average='micro'),\n",
    "                     'regression': f1_score(y_valid4, y_pred_4, average='micro')},\n",
    "                      index = ['F1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d1c333d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drop_20</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.169637</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.172277</td>\n",
       "      <td>0.180858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Drop_20      mean    median  regression\n",
       "F1_score  0.169637  0.164356  0.172277    0.180858"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433fa30",
   "metadata": {},
   "source": [
    "In general, data_4 stands out as the best-performing dataset for the decision tree algorithm, as it retains more information from the original dataset. Consequently, data_4 will be selected for future training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e0fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data\n",
    "x_train4.to_csv('final_x_train.csv',index =False)\n",
    "y_train4.to_csv('final_y_train.csv',index =False)\n",
    "x_valid4.to_csv('final_x_valid.csv',index =False)\n",
    "y_valid4.to_csv('final_y_valid.csv',index =False)\n",
    "x_test4.to_csv('final_x_test.csv',index =False)\n",
    "y_test4.to_csv('final_y_test.csv',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6c23e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
